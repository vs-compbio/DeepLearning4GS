{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e551c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a3b62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # embedding code from cgpt\n",
    "# from gensim.models import Word2Vec\n",
    "# import numpy as np\n",
    "\n",
    "# # Generate random SNP data\n",
    "# np.random.seed(42)\n",
    "# snp_data = np.random.choice(['A', 'T', 'G', 'C'], size=(300, 100000))\n",
    "\n",
    "# # Train Word2Vec model\n",
    "# embedding_dim = 100  # Dimensionality of the embedding space\n",
    "# window_size = 5  # Context window size for Word2Vec\n",
    "# model = Word2Vec(snp_data, size=embedding_dim, window=window_size, min_count=1, workers=4)\n",
    "\n",
    "# # Get SNP embeddings for each genotype\n",
    "# snp_embeddings = np.zeros((300, embedding_dim))\n",
    "\n",
    "# for i, genotype in enumerate(snp_data):\n",
    "#     embeddings = []\n",
    "#     for snp in genotype:\n",
    "#         if snp in model.wv:\n",
    "#             embeddings.append(model.wv[snp])\n",
    "#     if len(embeddings) > 0:\n",
    "#         snp_embeddings[i] = np.mean(embeddings, axis=0)\n",
    "\n",
    "# # Print the shape of SNP embeddings\n",
    "# print(snp_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed49e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b4963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from keras.activations import relu, elu, linear, softmax\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "#from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from keras.losses import mean_squared_error, categorical_crossentropy, logcosh\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb9583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, Nadam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62e100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Device name: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check available devices\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if len(devices) > 0:\n",
    "    print(\"GPU is available\")\n",
    "    for device in devices:\n",
    "        print(\"Device name:\", device.name)\n",
    "else:\n",
    "    print(\"No GPU is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab57f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processig data\n",
    "def data_preprocess(dataset,target): \n",
    "    # Scaled data\n",
    "    min_max_scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    np_scaled = min_max_scaler.fit_transform(dataset)\n",
    "    X = pd.DataFrame(np_scaled)\n",
    "    \n",
    "    target_edit = pd.Series(target).values\n",
    "    target_edit = target_edit.reshape(-1,1)\n",
    "    np_scaled = min_max_scaler.fit_transform(target_edit)\n",
    "    Y = pd.DataFrame(np_scaled)\n",
    "    # Split data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)\n",
    "      \n",
    "    scaler = StandardScaler()\n",
    "    # Fit only to the training data\n",
    "    scaler.fit(X_train)\n",
    "        \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a74f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_correlation(pred, y_test, target):\n",
    "    pred = pred.reshape((-1,1))\n",
    "    target_edit = pd.Series(target).values\n",
    "    target_edit = target_edit.reshape(-1,1)\n",
    "    min_max_scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    np_scaled = min_max_scaler.fit_transform(target_edit)\n",
    "    target_pred = min_max_scaler.inverse_transform(pred)\n",
    "    target_orig = min_max_scaler.inverse_transform(y_test)\n",
    "    target_orig = target_orig[:,0]\n",
    "    target_orig = pd.Series(target_orig)\n",
    "    target_pred = target_pred[:,0]\n",
    "    target_pred = pd.Series(target_pred)\n",
    "    cor1 = target_orig.corr(target_pred, method='pearson')\n",
    "    return cor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564d224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron\n",
    "def multi_layer_perceptron(X_train, y_train, X_test, y_test):\n",
    "    mlp = MLPRegressor(max_iter=200, early_stopping = True)   #---------\n",
    "    #parameter spce for mlp\n",
    "    parameter_space = {\n",
    "        'hidden_layer_sizes': [(19,19,19), (19,38,19), (19, 38, 38, 19), (20, 20, 40, 40, 20), (38,38,38,19),  (50, 50, 38), (90,90,90), (120,90,90)],\n",
    "        'activation': ['tanh', 'relu', 'linear', 'identity', 'logistic'],\n",
    "        'solver': ['sgd', 'adam','lbfgs'],\n",
    "        'alpha': [0.001, 0.05, 0.4], # regularization\n",
    "        'learning_rate': ['constant','adaptive']\n",
    "    }\n",
    "    \n",
    "    # Grid search with 5 fold cross validation\n",
    "    clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5)\n",
    "    clf.fit(X_train, y_train.values.ravel())\n",
    "    \n",
    "    # Best parameter set\n",
    "    print('Best parameters found:\\n', clf.best_params_)\n",
    "        \n",
    "    # All results\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    \n",
    "    print(clf.best_params_)   \n",
    "    \n",
    "    # Predict for test data\n",
    "    pred = clf.predict(X_test)\n",
    "    scores = (clf.score(X_test,y_test))\n",
    "    return scores, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb76c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cnn model to call for grid search\n",
    "def baseline_model(nSNP):\n",
    "    def create_conv_NN():\n",
    "        nStride=3  # stride between convolutions\n",
    "        nFilter=64 # filters\n",
    "        model_cnn = Sequential()\n",
    "    \n",
    "        # add convolutional layer with l1 and l2 regularization\n",
    "        model_cnn.add(Conv1D(nFilter, kernel_size=3, strides=nStride, input_shape=(nSNP,1), kernel_regularizer='l1_l2'))\n",
    "        model_cnn.add(Conv1D(nFilter, kernel_size=3, activation='relu'))\n",
    "        # dropout added for regularization\n",
    "        model_cnn.add(Dropout(0.2))\n",
    "        # add pooling layer: takes maximum of two consecutive values\n",
    "        model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "        # Solutions above are linearized to accommodate a standard layer\n",
    "        model_cnn.add(Flatten())\n",
    "        model_cnn.add(Dense(64))\n",
    "        # activation layer\n",
    "        model_cnn.add(Activation('relu'))\n",
    "        model_cnn.add(Dense(32))\n",
    "        model_cnn.add(Activation('linear'))\n",
    "        model_cnn.add(Dense(1)) \n",
    "        \n",
    "        # Model Compiling \n",
    "        model_cnn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        return model_cnn\n",
    "    return create_conv_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6197c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network\n",
    "def cnn(X_train, y_train, X_test, y_test):\n",
    "    # need this to match dimensions\n",
    "    X2_train = np.expand_dims(X_train, axis=2) \n",
    "    X2_test = np.expand_dims(X_test, axis=2) \n",
    "    \n",
    "    nSNP=X_train.shape[1] \n",
    "    early_stopping = EarlyStopping()\n",
    "     \n",
    "    #build cnn regressor\n",
    "    cnn = KerasRegressor(build_fn=baseline_model(nSNP), verbose=1)\n",
    "    #hyper parameters\n",
    "    batch_size = [64, 128]\n",
    "    epochs = [150, 200]\n",
    "    \n",
    "    #parameter space\n",
    "    param_grid = dict(epochs=epochs, batch_size = batch_size)\n",
    "    # Grid search with 5 fold cross validation\n",
    "    grid = GridSearchCV(estimator=cnn, param_grid=param_grid, cv=5)\n",
    "    #fit training data\n",
    "    grid_result = grid.fit(X2_train, y_train, validation_split=0.2, callbacks=[early_stopping])\n",
    "    grid_result = grid.fit(X2_train, y_train)\n",
    "    #get best model\n",
    "    best_params = grid_result.best_params_\n",
    "    print(best_params)\n",
    "    \n",
    "    # predict for test set     \n",
    "    pred = grid.predict(X2_test)\n",
    "    score = (grid.score(X2_test,y_test))\n",
    "    \n",
    "    return score, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe45b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average of items in a list\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "454dd419",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/uufs/chpc.utah.edu/common/home/akaundal-group3/Vishal/NAM_dat.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef876571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2\n",
       "2    2\n",
       "3    2\n",
       "4    2\n",
       "Name: SpringWheatNAM_tag_268865, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[1:5, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dbccdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime elapsed: \u001b[39m\u001b[38;5;124m\"\u001b[39m,end \u001b[38;5;241m-\u001b[39m start)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Multilayer perceptron\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMLP:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     scores_mlp, pred_mlp \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_layer_perceptron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     cor_m \u001b[38;5;241m=\u001b[39m cal_correlation(pred_mlp, y_test, target)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrelation for MLP \u001b[39m\u001b[38;5;124m'\u001b[39m,cor_m)\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mmulti_layer_perceptron\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Grid search with 5 fold cross validation\u001b[39;00m\n\u001b[1;32m     14\u001b[0m clf \u001b[38;5;241m=\u001b[39m GridSearchCV(mlp, parameter_space, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Best parameter set\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters found:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, clf\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/concurrent/futures/_base.py:445\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #start time\n",
    "    start = time.time()\n",
    "    \n",
    "    #data = pd.read_csv('/uufs/chpc.utah.edu/common/home/akaundal-group3/Vishal/NAM_dat.csv', header=0)\n",
    "    dataset=data.iloc[:,19:]\n",
    "    target=data.iloc[:, 4]\n",
    "    \n",
    "    #preprocess data\n",
    "    X_train, y_train, X_test, y_test = data_preprocess(dataset, target)\n",
    "    \n",
    "    #store all cor and accuracy values for mlp\n",
    "    cor_mlp = []\n",
    "    acc_mlp = []\n",
    "    #store all cor and accuracy values for mlp\n",
    "    cor_cnn = []\n",
    "    acc_cnn = []\n",
    "    \n",
    "    #200 iterations\n",
    "    for i in range(200):\n",
    "        # Multilayer perceptron\n",
    "        print('MLP:')\n",
    "        scores_mlp, pred_mlp = multi_layer_perceptron(X_train, y_train, X_test, y_test)\n",
    "        cor_m = cal_correlation(pred_mlp, y_test, target)\n",
    "        print('Correlation for MLP ',cor_m)\n",
    "        cor_mlp.append(cor_m)\n",
    "        acc_mlp.append(scores_mlp)\n",
    "    \n",
    "        # CNN\n",
    "        #print('CNN:')\n",
    "       # scores_cnn, pred_cnn = cnn(X_train, y_train, X_test, y_test)\n",
    "       # cor_c = cal_correlation(pred_cnn, y_test, target)\n",
    "       # print('Correlation for CNN ',cor_c)\n",
    "       # cor_cnn.append(cor_c)\n",
    "       # acc_cnn.append(scores_mlp)\n",
    "    \n",
    "    #average values\n",
    "    print(\"Average accuracy for MLP: \", Average(acc_mlp))\n",
    "    print(\"Average correlation for MLP: \", Average(cor_mlp))\n",
    "   # print(\"Average accuracy for CNN: \", Average(acc_cnn))\n",
    "   # print(\"Average correlation for CNN: \", Average(cor_cnn))\n",
    "    \n",
    "    #end time\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed: \",end - start)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afcdcddd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241m.\u001b[39mlist_devices():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device\u001b[38;5;241m.\u001b[39mname:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(device\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_session' is not defined"
     ]
    }
   ],
   "source": [
    "for device in tf_session.list_devices():\n",
    "    if 'GPU' in device.name:\n",
    "        print(device.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0658fae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs and their device placement:\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Device placement: /device:GPU:0\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "Device placement: /device:GPU:0\n",
      "PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')\n",
      "Device placement: /device:GPU:0\n",
      "PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')\n",
      "Device placement: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 12:50:04.872022: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 12:50:09.429765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 317 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:a3:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.432011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:1 with 309 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:a4:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.434173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:2 with 309 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c3:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.436309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:3 with 349 MB memory:  -> device: 3, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c4:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.469172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 317 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:a3:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.471126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:1 with 309 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:a4:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.473052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:2 with 309 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c3:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.474942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:3 with 349 MB memory:  -> device: 3, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c4:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.507254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 317 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:a3:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.509193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:1 with 309 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:a4:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.511308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:2 with 309 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c3:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.513211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:3 with 349 MB memory:  -> device: 3, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c4:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.545180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 317 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:a3:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.547090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:1 with 309 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:a4:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.548994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:2 with 309 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c3:00.0, compute capability: 8.0\n",
      "2024-02-18 12:50:09.550868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:3 with 349 MB memory:  -> device: 3, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c4:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Available GPUs and their device placement:\")\n",
    "for gpu in gpus:\n",
    "    print(gpu)\n",
    "    print(\"Device placement:\", tf.test.gpu_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed77c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
